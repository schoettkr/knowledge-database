#+TITLE: Operating Systems - Linear Memory Model
#+DATE: 2019-05-01 10:00
#+HUGO_TAGS: uni os
#+HUGO_BASE_DIR: ../../../
#+HUGO_SECTION: uni/os
#+HUGO_DRAFT: false
#+HUGO_AUTO_SET_LASTMOD: true

* Intro
When a program is executed the instruction are present in memory/storage and then the CPU loads those instructions and executes them (program counter, Assembler). Hardware is able to deal with absolute (direct) and relative adressing modes.
- absolute (direct) addressing = memory locations of operands are specified as a whole and completely
- relative addressing = memory locations are specified relative to the current PC (program counter) via on offset that is added to the address of the next instruction
  
* Address Space
Primary memory (RAM) is constituted via *adress spaces*
- address space = a range of discrete memory addresses
- logical address space = address(es) generated by a CPU and seen by programs
- physical address space = address(es) that are seen by the memory unit and used to access memory units

Virtual addresses are mapped with physical addresses by the MMU. In early computers logical and physical addresses corresponded, but since the introduction of virtual memory most application programs do not have a knowledge of physical addresses.

In this context the following challenges are faced:
- memory management /in/ a linear address space (this chapter/post)
- mapping logical to physical addresses (chapter 06;next post)
- memory virtualization (chapter 07)
  
* Management in a (linear) Address Space
From the application point of view (also OS intern) an address space is a *linear* list of addresses. However such linear address space is commonly not used as a whole but rather separated into memory areas (Speicherbereiche). The memory areas in an adress space are a limited resource and therefore require management. This is where the concepts of allocating and releasing memory come into place.\\
[[/knowledge-database/images/mm-concepts.png]]
** Concepts of Memory Management
Usually memory gets allocated as multiples of fixed elementary sizes. So allocations are rounded up to the next multiple and this leads to memory locations that are marked as used but don't get used. For example, memory can only be provided to programs in chunks divisible by 4, 8 or 16, and as a result if a program requests perhaps 29 bytes, it will actually get a chunk of 32 bytes. When this happens, the excess memory goes to waste. In this scenario, the unusable memory is contained within an allocated region. This arrangement, termed fixed partitions, suffers from inefficient memory use - any process, no matter how small, occupies an entire partition. *This waste is called internal fragmentation*.\\
*External fragmentation* arises when free memory is separated into small blocks and is interspersed by allocated memory. It is a weakness of certain storage allocation algorithms, when they fail to order memory used by programs efficiently. The result is that, although free storage is available, it is effectively unusable because it is divided into pieces that are too small individually to satisfy the demands of the application. The term "external" refers to the fact that the unusable storage is outside the allocated regions.

** Memory Occupation Information
Information about memory (whether its free/used/occupied) is usually stored in a vector or a table. These informations are either stored /seperatedly/ (requires additional memory) or /integrated/ in the managed memory area.
*** Vector Representation
- a continous data structure that holds information about the complete memory space 
[[/knowledge-database/images/memory-vector.png]]
*** Table Representation
- information is stored about a specific state
- slides: es werden nur Informationen ueber einen Zustand gespeichert (Belegungstabelle oder Freitabelle)

Separated table representation (can be sorted by adress/length)\\
[[/knowledge-database/images/sep-table-rep.png]]

Integrated table representation\\
[[/knowledge-database/images/int-table-rep.png]]
- memory areas identify themselves
- begin with length specification
- include pointer to next element
- sorting by length/adress is possible as well
  
*** Selection Strategies
When the process of reserving memory isn't determined automatically/by default then there needs to be some kind of strategy.
**** First Fit
[[/knowledge-database/images/first-fit.png]]
- the list (sorted by address) is run through from the beginning and the first chunk of memory that is big enough is selected
- low search effort
- segmentation of memory (external fragmentation)
- concentration of used memory at the beginning (can lead to higher search effort down the line)
**** Next Fit
[[/knowledge-database/images/next-fit.png]]
- list is traversed in a cyclic manner
- search starts where the last occupation happened
- similar to first first but avoids the problem of concentration at the beginning of the memory chunk
**** Best Fit
[[/knowledge-database/images/best-fits.png]]
- the *smallest* sufficient chunk is chosen
- when the list is sorted by address it has to be completely traversed, that's why sorting by size should be favored in this case
- improved memory utilization because smaller allocation requests are fulfilled by using small chunks instead of sratching of larger chunks
- however this tends to create very small chunks which then might not be used at all (external fragmentation)
**** Nearest Fit
[[/knowledge-database/images/nearest-fit.png]]
- a desired adress is passed for the requested chunk
- the algorithm then begins a first-fit search starting at the requested address
  
*** Premanufactured Memory (Vorkonfektionieren)
The search effort for arbitrary releases and allocations is $\mathcal(O)(n)$. This effort can be reduced by "premanufactured" (vorkonfektionierte) chunks, meaning that the statistically most-requested sizes should be kept ready (Konfektionsware) which can then be accessed via a sorted binary tree.\\
[[/knowledge-database/images/konfekt-memory1.png]]
[[/knowledge-database/images/konfekt-memory2.png]]
*** Memory Release
[[/knowledge-database/images/memory-release.png]]\\
The memory release can be optimized:
[[/knowledge-database/images/memory-release2.png]]\\

** Examples of Concrete Methods
*** Ring Buffer / Queue Method
- allocation and releasing in same order (FIFO)
- chunks are of equal size
- no searching
- no external fragmentation, but internal fragmentation is possible
- automatic, instant reintegration
- real world example: eg buffer for device drivers (Uebergabepuffer bei Geraetetreibern)
[[/knowledge-database/images/ringbuffer.png]]
*** Stack Method
[[/knowledge-database/images/stack.png]]
- allocating and releasing happens in reverse order (LIFO)
- chunks of arbitrary sizes
- no searching
- automatic, instant reintegration
- real world example: eg local variables of a program
*** Vector Method
[[/knowledge-database/images/vector.png]]
- allocating and releasing in arbitrary order
- chunk size = $k * $  Grundeinheit
- search for the first fitting chunk
- internal and external fragmentation
- automatic, instant reintegration
- real world example: eg kernel storage for very small systems
*** Boundary Tag Method
[[/knowledge-database/images/boundary-tag.png]]
- chunks are tagged
- chunks are chained by size
- allows allocation of arbitrary sizes
- occupation data and chunk managment is integrated (doubly linked list)
- external fragmentation
- explicitly initiated, instant reintegration
  - makes use of the length arrays to check the neighbours
  - sorted insertion into the linked list
- real world example: eg =malloc= in the GNU C library builds on a boundary tag method (but much more complex)
*** Buddy System
- memory consists of $2^{k_{max}}$ units
- smaller chunks are a result of continous halving/bisection of larger chunks
- smaller chunks that were produced together are merged back into larger chunks when released
- allocating and releasing in arbitrary order
- allocation in chunk sizes of $2^0, 2^1, 2^2, ..., 2^k$
- not much searching
- internal and external fragmentation
- release explicitly carried out
  
Sequence of Allocation
- round up to next power of two
- access of first available chunk in the list
- if list is empty (recursive):
  - access list of next bigger size
  - remove a chunk
  - half that chunk
  - insert the latter half (back?) into the list
 
Sequence of Release
- determine buddy (the other half)
- if buddy is occupied \rightarrow insert the released chunk into its list 
- if buddy is free \rightarrow merge buddies
- repeat this process until there's an occupied buddy or arrived at largest size
- fast operations
- adapts to requirements profile
- relatively high internal fragmentation
- after initial phase there're usually little division and merging processes
- real world example: eg =jemalloc()= (FreeBSD)
[[/knowledge-database/images/buddy-system.png]]
-------
Good and concise overview https://medium.com/cracking-the-data-science-interview/how-operating-systems-work-10-concepts-you-should-know-as-a-developer-8d63bb38331f
